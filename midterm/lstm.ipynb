{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D77j26rFe0P0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d2457d-14a3-4e8b-f03a-865d788f9aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "import json\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3ux_rnT9dN_",
        "outputId": "5c70879a-5621-4a7a-9e8a-3a0601d5eb1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load data from .npy file\n",
        "data = np.load(\"/content/drive/MyDrive/BrainTeaser/word_puzzle.npy\", allow_pickle=True)[()]\n",
        "\n",
        "questions = [entry['question'] for entry in data]\n",
        "options = [entry['choice_list'] for entry in data]  # List of option lists\n",
        "correct_indices = [entry['label'] for entry in data]\n",
        "# data\n",
        "\n",
        "# Rest of the code remains mostly the same"
      ],
      "metadata": {
        "id": "ZLmyDWQ6e-uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = '/content/csqa_rs_train.jsonl'  # Replace with the path to your JSON Lines file\n",
        "\n",
        "# # Read the entire file into a list using a list comprehension\n",
        "# with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#     data = [json.loads(line) for line in file]\n",
        "# questions = [entry['question']['stem'] for entry in data]\n",
        "\n",
        "# options = []\n",
        "# for entry in data:\n",
        "#     choices = [choice['text'] for choice in entry['question']['choices']]\n",
        "#     options.append(choices)\n",
        "\n",
        "# correct_indices = [ord(entry['answerKey']) - ord('A') +1 for entry in data]\n",
        "# print(data[0])\n",
        "# print(questions[0])\n",
        "# print(options[0])\n",
        "# print(correct_indices)\n"
      ],
      "metadata": {
        "id": "hvdNVkRkG1F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "# def clean_text(text):\n",
        "#     # Remove extra spaces\n",
        "#     text = re.sub(r'\\s+', ' ', text)  # Replace consecutive spaces with a single space\n",
        "\n",
        "#     # Remove backslashes\n",
        "#     text = text.replace('\\n', '')\n",
        "\n",
        "#     return text\n",
        "\n",
        "# # Example text with extra spaces and backslashes\n",
        "# example_text = \"This is some     example    text with \\\\ extra spaces and backslashes.    \"\n",
        "\n",
        "# # Clean the text\n",
        "# cleaned_text = clean_text(example_text)\n",
        "\n",
        "# # Print the cleaned text\n",
        "# print(cleaned_text)"
      ],
      "metadata": {
        "id": "jjD7A2ihIXwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(data)\n",
        "# print(df.shape)"
      ],
      "metadata": {
        "id": "amSQ2TNcVRYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(questions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(questions)\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
      ],
      "metadata": {
        "id": "WIcaideOfD2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert correct answer indices to one-hot encoding\n",
        "num_options = len(options[0])  # Assuming all questions have the same number of options\n",
        "encoded_correct_answers = tf.one_hot(correct_indices, num_options)\n",
        "\n"
      ],
      "metadata": {
        "id": "e9MjZ-qWfG6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_options, activation='softmax'))  # Number of output units equals the number of options\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, encoded_correct_answers, epochs=10, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IDjxeW6fJSg",
        "outputId": "88dde02f-b20f-4a55-ea32-48a4d3f67c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 234ms/step - loss: 1.3269 - accuracy: 0.3348 - val_loss: 1.2270 - val_accuracy: 0.3600\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 4s 266ms/step - loss: 1.2249 - accuracy: 0.3529 - val_loss: 1.1240 - val_accuracy: 0.3600\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 2s 134ms/step - loss: 1.2192 - accuracy: 0.3009 - val_loss: 1.1398 - val_accuracy: 0.3600\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 2s 127ms/step - loss: 1.1910 - accuracy: 0.3529 - val_loss: 1.1279 - val_accuracy: 0.3600\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 1s 96ms/step - loss: 1.1719 - accuracy: 0.3575 - val_loss: 1.1078 - val_accuracy: 0.3600\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 1s 68ms/step - loss: 1.1690 - accuracy: 0.3213 - val_loss: 1.1260 - val_accuracy: 0.3600\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 1.1118 - accuracy: 0.3688 - val_loss: 1.1155 - val_accuracy: 0.3600\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 1.0960 - accuracy: 0.3733 - val_loss: 1.1023 - val_accuracy: 0.3600\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 1.1166 - accuracy: 0.3529 - val_loss: 1.1059 - val_accuracy: 0.3600\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 1.0897 - accuracy: 0.3688 - val_loss: 1.1185 - val_accuracy: 0.3600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3fbe524d90>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference for a question and options from the dataset\n",
        "example_index = 2  # You can change this index to select a specific example from your dataset\n",
        "given_question = [questions[example_index]]\n",
        "given_options = options[example_index]\n",
        "\n",
        "# Tokenize and pad the given question\n",
        "given_question_sequence = tokenizer.texts_to_sequences(given_question)\n",
        "given_padded_sequence = pad_sequences(given_question_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict the option probabilities for the given question\n",
        "predicted_option_probs = model.predict(given_padded_sequence)\n",
        "\n",
        "# Get the index of the predicted correct option\n",
        "predicted_option_index = np.argmax(predicted_option_probs)\n",
        "predicted_option = given_options[predicted_option_index]\n",
        "\n",
        "# Print the predicted option and its probability\n",
        "print(\"Given Question:\", given_question[0])\n",
        "print(\"Given Options:\", given_options)\n",
        "print(\"Predicted Option Index:\", predicted_option_index)\n",
        "print(\"Predicted Correct Option:\", predicted_option)\n",
        "print(\"Correct Index:\", correct_indices[example_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-yHpvC3hnxr",
        "outputId": "5cd1ae95-f13f-4b8a-d038-a42e3df495e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b40381eb400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 343ms/step\n",
            "Given Question: How do you spell COB in seven letters?\n",
            "Given Options: ['COBCOBB', 'SEE O BEE', 'COBBLER', 'None of above.']\n",
            "Predicted Option Index: 0\n",
            "Predicted Correct Option: COBCOBB\n",
            "Correct Index: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Load data from .npy file\n",
        "data = np.load(\"/content/drive/MyDrive/BrainTeaser/word_puzzle.npy\", allow_pickle=True)[()]\n",
        "\n",
        "questions = [entry['question'] for entry in data]\n",
        "options = [entry['choice_list'] for entry in data]  # List of option lists\n",
        "correct_indices = [entry['label'] for entry in data]\n",
        "\n",
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers using regex\n",
        "    # text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stop words or perform additional text cleaning if necessary\n",
        "    # Example:\n",
        "    # words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocess questions\n",
        "preprocessed_questions = [preprocess_text(question) for question in questions]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_questions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_questions)\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert correct answer indices to one-hot encoding\n",
        "num_options = 4  # Assuming each question has 4 options\n",
        "encoded_correct_answers = tf.one_hot(correct_indices, num_options)\n",
        "\n",
        "# Build and train the model (as in previous code)\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_options, activation='softmax'))  # Number of output units equals the number of options\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, encoded_correct_answers, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Example inference for a question and options from the dataset\n",
        "example_index = 0  # You can change this index to select a specific example from your dataset\n",
        "given_question = [preprocess_text(questions[example_index])]\n",
        "given_options = options[example_index]\n",
        "\n",
        "# Tokenize and pad the given question\n",
        "given_question_sequence = tokenizer.texts_to_sequences(given_question)\n",
        "given_padded_sequence = pad_sequences(given_question_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict the option probabilities for the given question\n",
        "predicted_option_probs = model.predict(given_padded_sequence)\n",
        "\n",
        "# Get the index of the predicted correct option\n",
        "predicted_option_index = np.argmax(predicted_option_probs)\n",
        "predicted_option = given_options[predicted_option_index]\n",
        "\n",
        "# Print the predicted option, its probability, and the correct index\n",
        "print(\"Given Question:\", given_question[0])\n",
        "print(\"Given Options:\", given_options)\n",
        "print(\"Predicted Option Index:\", predicted_option_index)\n",
        "print(\"Predicted Correct Option:\", predicted_option)\n",
        "print(\"Correct Index:\", correct_indices[example_index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9cIkkO-lCmX",
        "outputId": "e38fe1cc-9ab4-4a85-c829-c7d584474fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 205ms/step - loss: 1.2821 - accuracy: 0.2919 - val_loss: 1.1119 - val_accuracy: 0.3800\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 7s 507ms/step - loss: 1.2072 - accuracy: 0.3710 - val_loss: 1.1467 - val_accuracy: 0.3600\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 4s 308ms/step - loss: 1.2009 - accuracy: 0.3145 - val_loss: 1.1260 - val_accuracy: 0.3600\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 4s 274ms/step - loss: 1.1877 - accuracy: 0.3439 - val_loss: 1.1279 - val_accuracy: 0.3800\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 2s 185ms/step - loss: 1.1790 - accuracy: 0.3190 - val_loss: 1.1288 - val_accuracy: 0.3600\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 2s 153ms/step - loss: 1.1213 - accuracy: 0.3710 - val_loss: 1.0940 - val_accuracy: 0.3800\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 2s 151ms/step - loss: 1.1124 - accuracy: 0.3529 - val_loss: 1.1259 - val_accuracy: 0.2600\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 2s 188ms/step - loss: 1.1012 - accuracy: 0.3348 - val_loss: 1.1034 - val_accuracy: 0.3600\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 2s 109ms/step - loss: 1.1000 - accuracy: 0.3665 - val_loss: 1.0992 - val_accuracy: 0.3600\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 1.0964 - accuracy: 0.3122 - val_loss: 1.1213 - val_accuracy: 0.2600\n",
            "1/1 [==============================] - 0s 337ms/step\n",
            "Given Question: how do you spell cow in thirteen letters ?\n",
            "Given Options: ['SEE O DOUBLE YOU.', 'SEE OH DEREFORD', 'COWCOWCOWCOWW', 'None of above.']\n",
            "Predicted Option Index: 2\n",
            "Predicted Correct Option: COWCOWCOWCOWW\n",
            "Correct Index: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_correct_answers = np.array(correct_indices)  # Convert to numpy array\n",
        "encoded_correct_answers = np.expand_dims(encoded_correct_answers, axis=-1)  # Expand dimensions to match logits\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=200, input_length=max_sequence_length))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))  # Added dropout\n",
        "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))  # Added dropout\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_options, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_sequences, encoded_correct_answers, epochs=20, batch_size=64, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "0gBghdlQgKyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92af7a1-0222-4b44-bf51-88776da95d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 22s 1s/step - loss: 1.3045 - accuracy: 0.3079 - val_loss: 1.1972 - val_accuracy: 0.3333\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 7s 985ms/step - loss: 1.2359 - accuracy: 0.3537 - val_loss: 1.1512 - val_accuracy: 0.3434\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 10s 2s/step - loss: 1.2382 - accuracy: 0.3079 - val_loss: 1.1526 - val_accuracy: 0.3434\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 7s 957ms/step - loss: 1.1703 - accuracy: 0.3232 - val_loss: 1.1235 - val_accuracy: 0.3333\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 10s 1s/step - loss: 1.1860 - accuracy: 0.3613 - val_loss: 1.1431 - val_accuracy: 0.3333\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 1.1213 - accuracy: 0.3791 - val_loss: 1.1300 - val_accuracy: 0.3434\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 1.0538 - accuracy: 0.4682 - val_loss: 1.1348 - val_accuracy: 0.3434\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 8s 1s/step - loss: 0.9578 - accuracy: 0.5013 - val_loss: 1.2199 - val_accuracy: 0.3535\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 7s 916ms/step - loss: 0.8307 - accuracy: 0.6056 - val_loss: 1.5341 - val_accuracy: 0.3333\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.6270 - accuracy: 0.7226 - val_loss: 1.5647 - val_accuracy: 0.3030\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 7s 867ms/step - loss: 0.5333 - accuracy: 0.7659 - val_loss: 1.5723 - val_accuracy: 0.3636\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.4084 - accuracy: 0.8473 - val_loss: 1.7699 - val_accuracy: 0.3535\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 6s 909ms/step - loss: 0.3170 - accuracy: 0.8982 - val_loss: 1.9653 - val_accuracy: 0.3535\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2535 - accuracy: 0.9109 - val_loss: 2.0198 - val_accuracy: 0.3232\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 6s 901ms/step - loss: 0.2111 - accuracy: 0.9211 - val_loss: 2.0210 - val_accuracy: 0.3636\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.2162 - accuracy: 0.9237 - val_loss: 2.2681 - val_accuracy: 0.3333\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 6s 795ms/step - loss: 0.1520 - accuracy: 0.9542 - val_loss: 2.2362 - val_accuracy: 0.3737\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.1290 - accuracy: 0.9669 - val_loss: 2.5920 - val_accuracy: 0.3131\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 6s 932ms/step - loss: 0.1268 - accuracy: 0.9593 - val_loss: 2.6633 - val_accuracy: 0.3131\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 9s 1s/step - loss: 0.1124 - accuracy: 0.9746 - val_loss: 3.0980 - val_accuracy: 0.3535\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3fa9f5ff40>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference for a question and options from the dataset\n",
        "example_index = 0  # You can change this index to select a specific example from your dataset\n",
        "given_question = [questions[example_index]]\n",
        "given_options = options[example_index]\n",
        "\n",
        "# Tokenize and pad the given question\n",
        "given_question_sequence = tokenizer.texts_to_sequences(given_question)\n",
        "given_padded_sequence = pad_sequences(given_question_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Predict the option probabilities for the given question\n",
        "predicted_option_probs = model.predict(given_padded_sequence)\n",
        "\n",
        "# Get the index of the predicted correct option\n",
        "predicted_option_index = np.argmax(predicted_option_probs)\n",
        "predicted_option = given_options[predicted_option_index]\n",
        "\n",
        "# Print the predicted option and its probability\n",
        "print(\"Given Question:\", given_question[0])\n",
        "print(\"Given Options:\", given_options)\n",
        "print(\"Predicted Option Index:\", predicted_option_index)\n",
        "print(\"Predicted Correct Option:\", predicted_option)\n",
        "print(\"Correct Index:\", correct_indices[example_index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMivq_52n28K",
        "outputId": "a24b399d-eb15-4c40-a73e-95f0dd1e7104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Given Question: How do you spell COW in thirteen letters?\n",
            "Given Options: ['SEE O DOUBLE YOU.', 'SEE OH DEREFORD', 'COWCOWCOWCOWW', 'None of above.']\n",
            "Predicted Option Index: 0\n",
            "Predicted Correct Option: SEE O DOUBLE YOU.\n",
            "Correct Index: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2mH9oXtVSrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBo5dlKkjJA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}